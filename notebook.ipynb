{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef36f535-4bdc-4e2b-a22a-179372324b26",
   "metadata": {},
   "source": [
    "![walmartecomm](walmartecomm.jpg)\n",
    "\n",
    "Walmart is the biggest retail store in the United States. Just like others, they have been expanding their e-commerce part of the business. By the end of 2022, e-commerce represented a roaring $80 billion in sales, which is 13% of total sales of Walmart. One of the main factors that affects their sales is public holidays, like the Super Bowl, Labour Day, Thanksgiving, and Christmas. \n",
    "\n",
    "In this project, you have been tasked with creating a data pipeline for the analysis of supply and demand around the holidays, along with conducting a preliminary analysis of the data. You will be working with two data sources: grocery sales and complementary data. You have been provided with the `grocery_sales` table in `PostgreSQL` database with the following features:\n",
    "\n",
    "# `grocery_sales`\n",
    "- `\"index\"` - unique ID of the row\n",
    "- `\"Store_ID\"` - the store number\n",
    "- `\"Date\"` - the week of sales\n",
    "- `\"Weekly_Sales\"` - sales for the given store\n",
    "\n",
    "Also, you have the `extra_data.parquet` file that contains complementary data:\n",
    "\n",
    "# `extra_data.parquet`\n",
    "- `\"IsHoliday\"` - Whether the week contains a public holiday - 1 if yes, 0 if no.\n",
    "- `\"Temperature\"` - Temperature on the day of sale\n",
    "- `\"Fuel_Price\"` - Cost of fuel in the region\n",
    "- `\"CPI\"` â€“ Prevailing consumer price index\n",
    "- `\"Unemployment\"` - The prevailing unemployment rate\n",
    "- `\"MarkDown1\"`, `\"MarkDown2\"`, `\"MarkDown3\"`, `\"MarkDown4\"` - number of promotional markdowns\n",
    "- `\"Dept\"` - Department Number in each store\n",
    "- `\"Size\"` - size of the store\n",
    "- `\"Type\"` - type of the store (depends on `Size` column)\n",
    "\n",
    "You will need to merge those files and perform some data manipulations. The transformed DataFrame can then be stored as the `clean_data` variable containing the following columns:\n",
    "- `\"Store_ID\"`\n",
    "- `\"Month\"`\n",
    "- `\"Dept\"`\n",
    "- `\"IsHoliday\"`\n",
    "- `\"Weekly_Sales\"`\n",
    "- `\"CPI\"`\n",
    "- \"`\"Unemployment\"`\"\n",
    "\n",
    "After merging and cleaning the data, you will have to analyze monthly sales of Walmart and store the results of your analysis as the `agg_data` variable that should look like:\n",
    "\n",
    "|  Month | Weekly_Sales  | \n",
    "|---|---|\n",
    "| 1.0  |  33174.178494 |\n",
    "|  2.0 |  34333.326579 |\n",
    "|  ... | ...  |  \n",
    "\n",
    "Finally, you should save the `clean_data` and `agg_data` as the csv files.\n",
    "\n",
    "It is recommended to use `pandas` for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765a0d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59fe49dc-cda5-4d22-bb10-49e94cdb6437",
   "metadata": {
    "collapsed": true,
    "customType": "sql",
    "dataFrameVariableName": "grocery_sales",
    "executionCancelledAt": null,
    "executionTime": 4242,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1748037064440,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "-- Write your SQL query here\nSELECT * FROM grocery_sales",
    "outputsMetadata": {
     "0": {
      "height": 316,
      "type": "dataFrame"
     }
    },
    "sqlCellMode": "dataFrame",
    "sqlSource": {
     "integrationId": "89e17161-a224-4a8a-846b-0adc0fe7a4b1",
     "type": "integration"
    }
   },
   "outputs": [],
   "source": [
    "# Define an extract function    \n",
    "def extract(store_sales, extra_data):\n",
    "    \"\"\"\n",
    "    Extracts data from the store_sales and extra_data files.\n",
    "    \n",
    "    Parameters:\n",
    "    store_sales (str): Path to the store sales CSV file.\n",
    "    extra_data (str): Path to the extra data parquet file.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the extracted data.\n",
    "    \"\"\"\n",
    "    # Read the store sales data\n",
    "    store_sales_df = pd.read_csv(store_sales)\n",
    "    \n",
    "    # Read the extra data\n",
    "    extra_data_df = pd.read_parquet(extra_data)\n",
    "    \n",
    "    # Merge the two DataFrames on 'index'\n",
    "    merged_df = store_sales_df.merge(extra_data_df, on='index')    \n",
    "    return merged_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d64ff1-a4ca-4a82-a8b4-e210244dedc1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 96,
    "lastExecutedAt": 1748037064536,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\nimport os\n\n# Extract function is already implemented for you \ndef extract(store_data, extra_data):\n    extra_df = pd.read_parquet(extra_data)\n    merged_df = store_data.merge(extra_df, on = \"index\")\n    return merged_df\n\n# Call the extract() function and store it as the \"merged_df\" variable\nmerged_df = extract(grocery_sales, \"extra_data.parquet\")\n\nprint(merged_df.head())",
    "outputsMetadata": {
     "0": {
      "height": 185,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/agg_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m extra_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/extra_data.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Call the extract() function and store it as the \"merged_df\" variable\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_sales\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mextract\u001b[1;34m(store_sales, extra_data)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mExtracts data from the store_sales and extra_data files.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mpd.DataFrame: DataFrame containing the extracted data.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Read the store sales data\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m store_sales_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_sales\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Read the extra data\u001b[39;00m\n\u001b[0;32m     17\u001b[0m extra_data_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(extra_data)\n",
      "File \u001b[1;32mc:\\Users\\davic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\davic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\davic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\davic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\davic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/agg_data.csv'"
     ]
    }
   ],
   "source": [
    "# Define the path to the store sales CSV file\n",
    "store_sales = \"/data/grocery_sales.csv\"  # Update this path to your actual raw sales data file\n",
    "extra_data = \"/data/extra_data.parquet\"\n",
    "\n",
    "# Call the extract() function and store it as the \"merged_df\" variable\n",
    "merged_df = extract(store_sales, extra_data)\n",
    "\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "964a5d2c-023f-4fe3-ba39-fdb104379b12",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 55,
    "lastExecutedAt": 1748037064591,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Check for missing values \nmerged_df.isna().sum()",
    "outputsMetadata": {
     "0": {
      "height": 550,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "b58c9056-ed38-425f-bc2c-17ceabd42d2b",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "0": [
          0,
          0,
          39,
          0,
          38,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          47,
          37,
          1,
          1
         ],
         "index": [
          "index",
          "Store_ID",
          "Date",
          "Dept",
          "Weekly_Sales",
          "IsHoliday",
          "Temperature",
          "Fuel_Price",
          "MarkDown1",
          "MarkDown2",
          "MarkDown3",
          "MarkDown4",
          "MarkDown5",
          "CPI",
          "Unemployment",
          "Type",
          "Size"
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "string"
          },
          {
           "name": "0",
           "type": "integer"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 17,
       "truncation_type": null
      },
      "text/plain": [
       "index            0\n",
       "Store_ID         0\n",
       "Date            39\n",
       "Dept             0\n",
       "Weekly_Sales    38\n",
       "IsHoliday        0\n",
       "Temperature      0\n",
       "Fuel_Price       0\n",
       "MarkDown1        0\n",
       "MarkDown2        0\n",
       "MarkDown3        0\n",
       "MarkDown4        1\n",
       "MarkDown5        1\n",
       "CPI             47\n",
       "Unemployment    37\n",
       "Type             1\n",
       "Size             1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {
      "application/com.datacamp.data-table.v2+json": {
       "status": "success"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values \n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6d3c25e2-e7d8-4c33-9be0-d45f03b2cf43",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1748037064642,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the transform() function with one parameter: \"raw_data\"\ndef transform(raw_data): \n    # fill missing numerical values with mean\n    raw_data.fillna(\n        {\n            'CPI': raw_data['CPI'].mean(), \n            'Weekly_Sales': raw_data['Weekly_Sales'].mean(), \n            'Unemployment': raw_data['Unemployment'].mean()\n        }, inplace=True\n    ) \n\n    # Convert Date column to date_time_type\n    raw_data[\"Date\"] = pd.to_datetime(raw_data[\"Date\"], format = \"%Y-%m-%d\") \n    \n    # Extract Month value from date\n    raw_data['Month'] = raw_data['Date'].dt.month \n\n    # Filter rows where weekly_sales > 10,000\n    raw_data = raw_data.loc[raw_data['Weekly_Sales'] > 10000, :] \n\n    # Filter for required columns \n    raw_data = raw_data.drop([\"index\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"], axis=1)\n\n    return raw_data\n"
   },
   "outputs": [],
   "source": [
    "# Create the transform() function with one parameter: \"raw_data\"\n",
    "def transform(raw_data): \n",
    "    # fill missing numerical values with mean\n",
    "    raw_data.fillna(\n",
    "        {\n",
    "            'CPI': raw_data['CPI'].mean(), \n",
    "            'Weekly_Sales': raw_data['Weekly_Sales'].mean(), \n",
    "            'Unemployment': raw_data['Unemployment'].mean()\n",
    "        }, inplace=True\n",
    "    ) \n",
    "\n",
    "    # Convert Date column to date_time_type\n",
    "    raw_data[\"Date\"] = pd.to_datetime(raw_data[\"Date\"], format = \"%Y-%m-%d\") \n",
    "    \n",
    "    # Extract Month value from date\n",
    "    raw_data['Month'] = raw_data['Date'].dt.month \n",
    "\n",
    "    # Filter rows where weekly_sales > 10,000\n",
    "    raw_data = raw_data.loc[raw_data['Weekly_Sales'] > 10000, :] \n",
    "\n",
    "    # Filter for required columns \n",
    "    raw_data = raw_data.drop([\"index\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"], axis=1)\n",
    "\n",
    "    return raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4b01ac49-bfb0-4a2f-b0c4-99cbb51aa838",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 375,
    "lastExecutedAt": 1748037065018,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "clean_data = transform(merged_df)\n# clean_data.drop(clean_data.columns[0], axis=1, inplace=True)\n\nprint(clean_data.head())",
    "outputsMetadata": {
     "0": {
      "height": 143,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store_ID  Dept  Weekly_Sales  IsHoliday         CPI  Unemployment  Month\n",
      "0         1     1      24924.50          0  211.096358      8.106000    2.0\n",
      "1         1    26      11737.12          0  211.096358      8.106000    2.0\n",
      "2         1    17      13223.76          0  211.096358      8.106000    2.0\n",
      "5         1    79      46729.77          0  211.096358      7.500052    2.0\n",
      "6         1    55      21249.31          0  211.096358      7.500052    2.0\n"
     ]
    }
   ],
   "source": [
    "clean_data = transform(merged_df)\n",
    "# clean_data.drop(clean_data.columns[0], axis=1, inplace=True)\n",
    "\n",
    "print(clean_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b19b15e3-6624-47a9-927f-d3f12fe8212d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1748037065066,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\ndef avg_weekly_sales_per_month(clean_data):\n    holiday_sales = clean_data[['Month', 'Weekly_Sales']]\n\n    holiday_sales = holiday_sales.groupby('Month').agg(Avg_Sales = ('Weekly_Sales', 'mean')).reset_index().round(2)\n    \n    return holiday_sales"
   },
   "outputs": [],
   "source": [
    "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\n",
    "def avg_weekly_sales_per_month(clean_data):\n",
    "    holiday_sales = clean_data[['Month', 'Weekly_Sales']]\n",
    "\n",
    "    holiday_sales = holiday_sales.groupby('Month').agg(Avg_Sales = ('Weekly_Sales', 'mean')).reset_index().round(2)\n",
    "    \n",
    "    return holiday_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fe875e27-b0cf-4e52-994e-4ae1fe6e8876",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1748037065119,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\nagg_data = avg_weekly_sales_per_month(clean_data)\n\nprint(agg_data.head())",
    "outputsMetadata": {
     "0": {
      "height": 143,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Month  Avg_Sales\n",
      "0    1.0   33174.18\n",
      "1    2.0   34333.33\n",
      "2    3.0   33220.89\n",
      "3    4.0   33392.37\n",
      "4    5.0   33339.89\n"
     ]
    }
   ],
   "source": [
    "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\n",
    "agg_data = avg_weekly_sales_per_month(clean_data)\n",
    "\n",
    "print(agg_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "921cb123-3153-4334-bdeb-9bb227fdc530",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1748037065166,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\ndef load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n    full_data.to_csv(full_data_file_path, index=False)\n    agg_data.to_csv(agg_data_file_path, index=False) "
   },
   "outputs": [],
   "source": [
    "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\n",
    "def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n",
    "    full_data.to_csv(full_data_file_path, index=False)\n",
    "    agg_data.to_csv(agg_data_file_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f518ad5c-214e-474b-80bd-827b0c0e1536",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 388,
    "lastExecutedAt": 1748037065555,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \nload(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')"
   },
   "outputs": [],
   "source": [
    "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \n",
    "load(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "61b5f58a-70cb-40b3-bdbe-20b4079276e3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 53,
    "lastExecutedAt": 1748037065610,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\ndef validation(file_path):\n    file_exists = os.path.exists(file_path) \n\n    if not file_exists:\n        raise Exception (f'There is no file at the path {file_path}')"
   },
   "outputs": [],
   "source": [
    "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\n",
    "def validation(file_path):\n",
    "    file_exists = os.path.exists(file_path) \n",
    "\n",
    "    if not file_exists:\n",
    "        raise Exception (f'There is no file at the path {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "df1659ff-41c4-4a92-9812-80c6eaa02b90",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1748037065658,
    "lastExecutedByKernel": "e91bd9f7-1c56-43ce-92d7-46f44e40656d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\nvalidation('clean_data.csv')\nvalidation('agg_data.csv')"
   },
   "outputs": [],
   "source": [
    "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\n",
    "validation('clean_data.csv')\n",
    "validation('agg_data.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
