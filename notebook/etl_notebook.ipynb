{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40375292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd \n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67670a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths \n",
    "store_sales = \"grocery_sales.csv\"\n",
    "extra_data = \"extra_data.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c44e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an extract function\n",
    "def extract(store_sales, extra_data):\n",
    "    \"\"\"\n",
    "    Extracts data from the store_sales and extra_data files.\n",
    "\n",
    "    Parameters:\n",
    "    store_sales (str): Path to the store sales CSV file.\n",
    "    extra_data (str): Path to the extra data parquet file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the extracted data.\n",
    "    \"\"\"\n",
    "    # Read the store sales data\n",
    "    store_sales = pd.read_csv(store_sales)\n",
    "\n",
    "    # Read the extra data\n",
    "    extra_data = pd.read_parquet(extra_data)\n",
    "\n",
    "    # Merge the two DataFrames on 'index'\n",
    "    merged_df = store_sales.merge(extra_data, on=\"index\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "795c647f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  Store_ID        Date  Dept  Weekly_Sales  IsHoliday  Temperature  \\\n",
      "0      0         1  2010-02-05     1      24924.50          0        42.31   \n",
      "1      1         1  2010-02-05    26      11737.12          0        42.31   \n",
      "2      2         1  2010-02-05    17      13223.76          0        42.31   \n",
      "3      3         1  2010-02-05    45         37.44          0        42.31   \n",
      "4      4         1  2010-02-05    28       1085.29          0        42.31   \n",
      "\n",
      "   Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5  \\\n",
      "0       2.572        0.0        0.0        0.0        0.0        0.0   \n",
      "1       2.572        0.0        0.0        0.0        0.0        0.0   \n",
      "2       2.572        0.0        0.0        0.0        0.0        0.0   \n",
      "3       2.572        0.0        0.0        0.0        0.0        0.0   \n",
      "4       2.572        0.0        0.0        0.0        0.0        0.0   \n",
      "\n",
      "          CPI  Unemployment  Type      Size  \n",
      "0  211.096358         8.106   3.0  151315.0  \n",
      "1  211.096358         8.106   3.0  151315.0  \n",
      "2  211.096358         8.106   3.0  151315.0  \n",
      "3  211.096358           NaN   3.0  151315.0  \n",
      "4  211.096358           NaN   3.0  151315.0  \n"
     ]
    }
   ],
   "source": [
    "# Call the extract function with file paths\n",
    "merged_data = extract(store_sales, extra_data)\n",
    "print(merged_data.head())  # Display the first few rows of the merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28f5b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index            0\n",
      "Store_ID         0\n",
      "Date            39\n",
      "Dept             0\n",
      "Weekly_Sales    38\n",
      "IsHoliday        0\n",
      "Temperature      0\n",
      "Fuel_Price       0\n",
      "MarkDown1        0\n",
      "MarkDown2        0\n",
      "MarkDown3        0\n",
      "MarkDown4        1\n",
      "MarkDown5        1\n",
      "CPI             47\n",
      "Unemployment    37\n",
      "Type             1\n",
      "Size             1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the merged DataFrame\n",
    "print(merged_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0005017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transform() function with one parameter: \"raw_data\"\n",
    "def transform(raw_data): \n",
    "    # fill missing numerical values with mean\n",
    "    raw_data.fillna(\n",
    "        {\n",
    "            'CPI': raw_data['CPI'].mean(), \n",
    "            'Weekly_Sales': raw_data['Weekly_Sales'].mean(), \n",
    "            'Unemployment': raw_data['Unemployment'].mean()\n",
    "        }, inplace=True\n",
    "    ) \n",
    "\n",
    "    # Convert Date column to date_time_type\n",
    "    raw_data[\"Date\"] = pd.to_datetime(raw_data[\"Date\"], format = \"%Y-%m-%d\") \n",
    "    \n",
    "    # Extract Month value from date\n",
    "    raw_data['Month'] = raw_data['Date'].dt.month \n",
    "\n",
    "    # Filter rows where weekly_sales > 10,000\n",
    "    raw_data = raw_data.loc[raw_data['Weekly_Sales'] > 10000, :] \n",
    "\n",
    "    # Filter for required columns \n",
    "    raw_data = raw_data.drop([\"index\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"], axis=1)\n",
    "\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d4d5290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store_ID  Dept  Weekly_Sales  IsHoliday         CPI  Unemployment  Month\n",
      "0         1     1      24924.50          0  211.096358      8.106000    2.0\n",
      "1         1    26      11737.12          0  211.096358      8.106000    2.0\n",
      "2         1    17      13223.76          0  211.096358      8.106000    2.0\n",
      "5         1    79      46729.77          0  211.096358      7.500052    2.0\n",
      "6         1    55      21249.31          0  211.096358      7.500052    2.0\n"
     ]
    }
   ],
   "source": [
    "# Call the transform function with the merged data\n",
    "clean_data = transform(merged_data)\n",
    "# Display the first few rows of the transformed DataFrame \n",
    "print(clean_data.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c988a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\n",
    "def avg_weekly_sales_per_month(clean_data):\n",
    "    holiday_sales = clean_data[['Month', 'Weekly_Sales']]\n",
    "\n",
    "    holiday_sales = holiday_sales.groupby('Month').agg(Avg_Sales = ('Weekly_Sales', 'mean')).reset_index().round(2)\n",
    "    \n",
    "    return holiday_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "619cbc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Month  Avg_Sales\n",
      "0    1.0   33174.18\n",
      "1    2.0   34333.33\n",
      "2    3.0   33220.89\n",
      "3    4.0   33392.37\n",
      "4    5.0   33339.89\n"
     ]
    }
   ],
   "source": [
    "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\n",
    "agg_data = avg_weekly_sales_per_month(clean_data)\n",
    "\n",
    "print(agg_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f5d36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\n",
    "def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n",
    "    full_data.to_csv(full_data_file_path, index=False)\n",
    "    agg_data.to_csv(agg_data_file_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fbf105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \n",
    "load(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0539df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\n",
    "def validation(file_path):\n",
    "    file_exists = os.path.exists(file_path) \n",
    "\n",
    "    if not file_exists:\n",
    "        raise Exception (f'There is no file at the path {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d068b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\n",
    "validation('clean_data.csv')\n",
    "validation('agg_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
